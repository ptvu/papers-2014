{
 "metadata": {
  "name": "",
  "signature": "sha256:14d2f5db149a1aed4be579b816c551b3fcd1066c9aa502c2f26307084a1bee3d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk\n",
      "import csv\n",
      "import re\n",
      "from cookielib import CookieJar\n",
      "cj = CookieJar()\n",
      "from bs4 import BeautifulSoup\n",
      "'''\n",
      "INPUT PARAMETERS\n",
      "'''\n",
      "uri = \"http://export.arxiv.org/rss/cs\"\n",
      "all_docs_words=set({})\n",
      "class RSS_item():\n",
      "  def __init__(self, eltdict):\n",
      "    self.title = eltdict['title']\n",
      "    self.source= eltdict['source']\n",
      "    self.content=eltdict['content'].lower().replace('\\n',' ')\n",
      "    self.content = re.sub(r'[^a-z]', ' ', self.content)\n",
      "    print self.content\n",
      "    self.all_words={}\n",
      "\n",
      "    stopWords = set()\n",
      "    cr = csv.reader(open('stopwords.txt'))\n",
      "    words=[]\n",
      "    for word in cr:\n",
      "     stopWords.add(word[0])\n",
      "\n",
      "    for word in nltk.word_tokenize(self.content):\n",
      "      if word not in stopWords:\n",
      "        all_docs_words.add(word)\n",
      "        if self.all_words.has_key(word):\n",
      "          self.all_words[word]=self.all_words[word] + 1\n",
      "        else:\n",
      "          self.all_words[word]=1\n",
      "\n",
      "opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))\n",
      "html = opener.open(uri).read()\n",
      "dicts = [];\n",
      "all_docs = []\n",
      "soup = BeautifulSoup(html,'xml')\n",
      "for elt in soup.findAll('item'):\n",
      "  titlestr = elt.find('title').getText()\n",
      "  linkstr = elt.find('link').getText()\n",
      "  descstr = elt.find('description').getText()\n",
      "  eltdict = {};\n",
      "  eltdict['title']=titlestr.encode('utf-8')\n",
      "  eltdict['content']=descstr.encode('utf-8')\n",
      "  eltdict['source']=linkstr.encode('utf-8')\n",
      "  dicts.append(eltdict)\n",
      "to_write=\"\"\n",
      "all_link_str=''\n",
      "for doc in dicts:\n",
      "  all_docs.append(RSS_item(doc))\n",
      "  to_write = to_write+ doc['title']+u'\\n'\n",
      "  all_link_str=all_link_str + doc['source']+u'\\n'\n",
      "all_docs_words=list(all_docs_words)\n",
      "f = open('hyperlink.csv','w')\n",
      "f.write(all_link_str)\n",
      "f.close\n",
      "f = open('documentTitle.csv', 'w')\n",
      "f.write(to_write)\n",
      "f.close()\n",
      "f = open(\"wordlist.csv\",'w')\n",
      "f.write('\\n'.join(all_docs_words))\n",
      "f.close()\n",
      "f = open('sparseMatrix.csv','w')\n",
      "i = 1;\n",
      "to_write=''\n",
      "for doc in all_docs:\n",
      "  for word in doc.all_words:\n",
      "    if(doc.all_words[word]!=0):\n",
      "     to_write = to_write + i.__str__() +','+(all_docs_words.index(word)+1).__str__()+','+doc.all_words[word].__str__()+'\\n'\n",
      "  i += 1\n",
      "f.write(to_write)\n",
      "f.close()\n",
      "\n",
      "# for i in all_docs[0].all_words: print i + '\\t\\t' + all_docs[0].all_words[i].__str__()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " p this paper takes shenzhen futian comprehensive transportation junction as the case  and makes use of continuous multiple real time dynamic traffic information to carry out monitoring and analysis on spatial and temporal distribution of passenger flow under different means of transportation and service capacity of junction from multi dimensional space time perspectives such as different period and special period  virtual reality geographic information system is employed to present the forecasting result    p  \n",
        " p a big city visual analysis platform based on web virtual reality geographical information system  webvrgis  is presented  extensive model editing functions and spatial analysis functions are available  including terrain analysis  spatial analysis  sunlight analysis  traffic analysis  population analysis and community analysis    p  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " p this article investigates the swept rule of space time domain decomposition  an idea to break the latency barrier via communicating less often when explicitly solving time dependent pdes  the swept rule decomposes space and time among computing nodes in ways that exploit the domains of influence and the domain of dependency  making it possible to communicate once per many timesteps without redundant computation  the article presents simple theoretical analysis to the performance of the swept rule which then was shown to be accurate by conducting numerical experiments    p  \n",
        " p the effects of soft errors in processor cores have been widely studied in literature  on the contrary  little has been published about soft errors in uncore components  such as memory subsystem and i o controllers  in a system on chip  soc   in this work  we study how soft errors in uncore components affect system level behaviors  we have created a new mixed mode simulation platform that combines simulators at two different levels of abstraction and achieves       x speedup over rtl only simulation  using this platform  we present the first study of the system level impact of soft errors inside various uncore components of a large scale  multi core soc using the industrial grade  open source opensparc t  soc design  our results show that soft errors in uncore components can significantly impact system level reliability  we also demonstrate that uncore soft errors can create major challenges for traditional system level checkpoint based recovery techniques  to overcome such recovery challenges  we present a new replay recovery technique for uncore components belonging to the memory subsystem  for the l  cache controller and the dram controller components of opensparc t   our new technique reduces the probability that an application run results in an erroneous outcome due to soft errors by more than    x with only       and       chip level area and power impact  respectively    p  \n",
        " p given the extremely large pool of events and stories available  media outlets need to focus on a subset of issues and aspects to convey to their audience  outlets are often accused of exhibiting a systematic bias in this selection process  with different outlets portraying different versions of reality  however  in the absence of objective measures and empirical evidence  the direction and extent of systematicity remains widely disputed    p   p in this paper we propose a framework based on quoting patterns for quantifying and characterizing the degree to which media outlets exhibit systematic bias  we apply this framework to a massive dataset of news articles spanning the six years of obama s presidency and all of his speeches  and reveal that a systematic pattern does indeed emerge from the outlet s quoting behavior  moreover  we show that this pattern can be successfully exploited in an unsupervised prediction setting  to determine which new quotes an outlet will select to broadcast  by encoding bias patterns in a low rank space we provide an analysis of the structure of political media coverage  this reveals a latent media bias space that aligns surprisingly well with political ideology and outlet type  a linguistic analysis exposes striking differences across these latent dimensions  showing how the different types of media outlets portray different realities even when reporting on the same events  for example  outlets mapped to the mainstream conservative side of the latent space focus on quotes that portray a presidential persona disproportionately characterized by negativity    p  \n",
        " p i consider the effect of a finite sample size on the entropy of a sample of independent events  i propose formula for entropy which satisfies shannon s axioms  and which reduces to shannon s entropy when sample size is infinite  i discuss the physical meaning of the difference between two formulas  including some practical implications  such as maximum achievable channel utilization  and minimum achievable communication protocol overhead  for a given message size    p  \n",
        " p the problem of offline to online script conversion is a challenging and an ill posed problem  the interest in offline to online conversion exists because there are a plethora of robust algorithms in online script literature which can not be used on offline scripts  in this paper  we propose a method  based on heuristics  to extract online script information from offline bitmap image  we show the performance of the proposed method on a real sample signature offline image  whose online information is known    p  \n",
        " p our proposed method of random phase free holography using virtual convergence light can obtain large reconstructed images exceeding the size of the hologram  without the assistance of random phase  the reconstructed images have low speckle noise in the amplitude and phase only holograms  kinoforms   however  in low resolution holograms  we obtain a degraded image quality compared to the original image  we propose an iterative random phase free method with virtual convergence light to address this problem    p  \n",
        " p the ability to classify spoken speech based on the style of speaking is an important problem  with the advent of bpo s in recent times  specifically those that cater to a population other than the local population  it has become necessary for bpo s to identify people with certain style of speaking  american  british etc   today bpo s employ accent analysts to identify people having the required style of speaking  this process while involving human bias  it is becoming increasingly infeasible because of the high attrition rate in the bpo industry  in this paper  we propose a new metric  which robustly and accurately helps classify spoken speech based on the style of speaking  the role of the proposed metric is substantiated by using it to classify real speech data collected from over seventy different people working in a bpo  we compare the performance of the metric against human experts who independently carried out the classification process  experimental results show that the performance of the system using the novel metric performs better than two different human expert    p  \n",
        " p the cfg recognition problem is  given a context free grammar g and a string w of length n  decide if w can be obtained from g  this is the most basic parsing question and is a core computer science problem  valiant s parser from      solves the problem in o n          time using fast matrix multiplication  dozens of parsing algorithms have been proposed over the years  yet valiant s upper bound remains unbeaten  the best combinatorial algorithms have mildly subcubic o n    log   n   complexity    p   p evidence for the nonexistence of efficient algorithms was given in a result of lee  jacm      who showed that any algorithm for a more general parsing problem with running time o  g  n    eps    that works even in the unusual case that  g  omega n     can be converted into a surprising subcubic algorithm for boolean matrix multiplication  however  nothing was known for the more relevant case of constant size grammars    p   p in this work  we make the first progress on the constant size grammar case in    years  and prove that any improvement on valiant s algorithm  either in terms of runtime or by avoiding the inefficiencies of fast matrix multiplication  would imply a breakthrough algorithm for the k clique problem  given a graph of n nodes  decide if there are k that form a clique    p   p besides classifying the complexity of a fundamental problem  our reduction has led us to similar lower bounds for more modern and well studied cubic time problems for which faster algorithms are highly desirable in practice  rna folding  a central problem in computational biology  and dyck language edit distance  answering an open question of saha  focs        p  \n",
        " p nowadays  the majority of rss feeds provide incomplete information about their news items  the lack of information leads to engagement loss in users  we present a new automated system for improving the rss feeds  data quality  rss feeds provide a list of the latest news items ordered by date  therefore  it makes it easy for a web crawler to precisely locate the item and extract its raw content  then it identifies where the main content is located and extracts  main text corpus  relevant keywords  bigrams  best image and predicts the category of the item  the output of the system is an enhanced rss feed  the proposed system showed an average item data quality improvement from        to           p  \n",
        " p we consider the quantized consensus problem on undirected connected graphs with n nodes  and devise a protocol with fast convergence time to the set of consensus points  specifically  we show that when the edges of a static network are activated based on poisson processes with metropolis rates  the expected convergence time to the set of consensus points is at most o n   log n   we further show an upper bound of o n   log   n  for the expected convergence time of the same protocol over connected time varying networks  these bounds are better than all previous convergence times for randomized quantized consensus    p  \n",
        " p image registration for stack based hdr photography is challenging  if not properly accounted for  camera motion and scene changes result in artifacts in the composite image  unfortunately  existing methods to address this problem are either accurate  but too slow for mobile devices  or fast  but prone to failing  we propose a method that fills this void  our approach is extremely fast   under    ms on a commercial tablet for a pair of  mp images   and prevents the artifacts that arise from insufficient registration quality    p  \n",
        " p a better understanding of how people move in space is of fundamental importance for many areas such as prevention of epidemics  urban planning and wireless network engineering  to name a few  in this work  we explore a rank based approach for the characterization of human trajectories and unveil a visitation bias toward recently visited locations  we test our hypothesis against different empirical data of human mobility  also  we propose an extension to the preferential return mechanism to incorporate the new recency based mechanism    p  \n",
        " p we propose a totally corrective boosting algorithm with explicit cardinality regularization  the resulting combinatorial optimization problems are not known to be efficiently solvable with existing classical methods  but emerging quantum optimization technology gives hope for achieving sparser models in practice  in order to demonstrate the utility of our algorithm  we use a distributed classical heuristic optimizer as a stand in for quantum hardware  even though this evaluation methodology incurs large time and resource costs on classical computing machinery  it allows us to gauge the potential gains in generalization performance and sparsity of the resulting boosted ensembles  our experimental results on public data sets commonly used for benchmarking of boosting algorithms decidedly demonstrate the existence of such advantages  if actual quantum optimization were to be used with this algorithm in the future  we would expect equivalent or superior results at much smaller time and energy costs during training  moreover  studying cardinality penalized boosting also sheds light on why unregularized boosting algorithms with early stopping often yield better results than their counterparts with explicit convex regularization  early stopping performs suboptimal cardinality regularization  the results that we present here indicate it is beneficial to explicitly solve the combinatorial problem still left open at early termination    p  \n",
        " p the rapid growth of data volume and the accompanying congestion problems over the wireless networks have been critical issues to content providers  a novel technique  termed as coded cache  is proposed to relieve the burden  through creating coded multicasting opportunities  the coded cache scheme can provide extra performance gain over the conventional push technique that simply pre stores contents at local caches during the network idle period  but existing works on the coded caching scheme assumed the availability of an error free shared channel accessible by each user  this paper considers the more realistic scenario where each user may experience different link quality  in this case  the system performance would be restricted by the user with the worst channel condition  and the corresponding resource allocation schemes aimed at breaking this obstacles are developed  specifically  we employ the coded caching scheme in time division and frequency division transmission mode and formulate the sub optimal problems  power and bandwidth are allocated respectively to maximum the system throughput  the simulation results show that the throughput of the technique in wireless scenario will be limited and would decrease as the number of users becomes sufficiently large    p  \n",
        " p the emerging field of signal processing on graph plays a more and more important role in processing signals and information related to networks  existing works have shown that under certain conditions a smooth graph signal can be uniquely reconstructed from its decimation  i e   data associated with a subset of vertices  however  in some potential applications  e g   sensor networks with clustering structure   the obtained data may be a combination of signals associated with several vertices  rather than the decimation  in this paper  we propose a new concept of local measurement  which is a generalization of decimation  using the local measurements  a local set based method named iterative local measurement reconstruction  ilmr  is proposed to reconstruct bandlimited graph signals  it is proved that ilmr can reconstruct the original signal perfectly under certain conditions  the performance of ilmr against noise is theoretically analyzed  the optimal choice of local weights and a greedy algorithm of local set partition are given in the sense of minimizing the expected reconstruction error  compared with decimation  the proposed local measurement sampling and reconstruction scheme is more robust in noise existing scenarios    p  \n",
        " p a rigorous proof is presented that the number of comparisons of keys performed in the worst case by    tt heapsort   on any array of size  n  geq    is equal to       n            lg  frac n         varepsilon         s   n    e   n     min   lfloor  lg  n     rfloor           c    where    varepsilon    given by    varepsilon        lceil  lg     n     rceil    lg     n          lceil  lg     n     rceil    lg     n        is a function of   n   with the minimum value   and and the supremum value   delta        lg e    lg  lg e  approx                       s   n   is the sum of all digits of the binary representation of  n    e   n   is the exponent of     in the prime factorization of  n   and   c   is a binary function on the set of integers defined by   c       if  n  leq      lceil  lg n  rceil        and  c       otherwise  an algorithm that generates worst case input arrays of any size   n  geq     for    tt heapsort   is offered  the algorithm has been implemented in java  runs in  o  n  log n    time  and allows for precise experimental verification of the above formula    p  \n",
        " p the shannon capacity of a graph  g  is defined as  c g   sup  d geq     alpha g d     frac    d     where   alpha g   is the independence number of  g   the shannon capacity of the cycle  c    on     vertices was determined by lov   a sz in       but the shannon capacity of a cycle  c p  for general odd  p  remains one of the most notorious open problems in information theory  by prescribing stabilizers for the independent sets in  c p d  and using stochastic search methods  we show that   alpha c      geq         alpha c         geq         alpha c         geq       and   alpha c         geq       this leads to improved lower bounds on the shannon capacity of  c    and  c         c c    geq       frac        gt          and  c c       geq       frac        gt             p  \n",
        " p identity of a vehicle is done through the vehicle license plate by traffic police in general  au  tomatic vehicle license plate recognition has several applications in intelligent traffic management systems  the security situation across the globe and particularly in india demands a need to equip the traffic police with a system that enables them to get instant details of a vehicle  the system should be easy to use  should be mobile  and work    x    in this paper  we describe a mobile phone based  client server architected  license plate recognition system  while we use the state of the art image processing and pattern recognition algorithms tuned for indian conditions to automatically recognize non uniform license plates  the main contribution is in creating an end to end usable solution  the client application runs on a mobile device and a server application  with access to vehicle information database  is hosted centrally  the solution enables capture of license plate image captured by the phone camera and passes to the server  on the server the license plate number is recognized  the data associated with the number plate is then sent back to the mobile device  instantaneously  we describe the end to end system architecture in detail  a working prototype of the proposed system has been implemented in the lab environment    p  \n",
        " p big data on electronic records of social interactions allow approaching human behaviour and sociality from a quantitative point of view with unforeseen statistical power  mobile telephone call detail records  cdrs   automatically collected by telecom operators for billing purposes  have proven especially fruitful for understanding one to one communication patterns as well as the dynamics of social networks that are reflected in such patterns  we present an overview of empirical results on the multi scale dynamics of social dynamics and networks inferred from mobile telephone calls  we begin with the shortest timescales and fastest dynamics  such as burstiness of call sequences between individuals  and  zoom out  towards longer temporal and larger structural scales  from temporal motifs formed by correlated calls between multiple individuals to long term dynamics of social groups  we conclude this overview with a future outlook    p  \n",
        " p we present a novel deep recurrent neural network  rnn  model for acoustic modelling in automatic speech recognition  asr   we term our contribution as a tc dnn blstm dnn model  the model combines a deep neural network  dnn  with time convolution  tc   followed by a bidirectional long short term memory  blstm   and a final dnn  the first dnn acts as a feature processor to our model  the blstm then generates a context from the sequence acoustic signal  and the final dnn takes the context and models the posterior probabilities of the acoustic states  we achieve a      wer on the wall street journal  wsj  eval   task or more than    relative improvement over the baseline dnn models    p  \n",
        " p deep neural network  dnn  acoustic models have yielded many state of the art results in automatic speech recognition  asr  tasks  more recently  recurrent neural network  rnn  models have been shown to outperform dnns counterparts  however  state of the art dnn and rnn models tend to be impractical to deploy on embedded systems with limited computational capacity  traditionally  the approach for embedded platforms is to either train a small dnn directly  or to train a small dnn that learns the output distribution of a large dnn  in this paper  we utilize a state of the art rnn to transfer knowledge to small dnn  we use the rnn model to generate soft alignments and minimize the kullback leibler divergence against the small dnn  the small dnn trained on the soft rnn alignments achieved a      wer on the wall street journal  wsj  eval   task compared to a baseline      wer or more than     relative improvement    p  \n",
        " p this paper describes a new feature set for use in the recognition of on line handwritten devanagari script based on fuzzy directional features  experiments are conducted for the automatic recognition of isolated handwritten character primitives  sub character units   initially we describe the proposed feature set  called the fuzzy directional features  fdf  and then show how these features can be effectively utilized for writer independent character recognition  experimental results show that fdf set perform well for writer independent data set at stroke level recognition  the main contribution of this paper is the introduction of a novel feature set and establish experimentally its ability in recognition of handwritten devanagari script    p  \n",
        " p conditional random fields  crf  have been widely used in a variety of computer vision tasks  conventional crfs typically define edges on neighboring image pixels  resulting in a sparse graph such that efficient inference can be performed  however  these crfs fail to model long range contextual relationships  fully connected crfs have thus been proposed  while there are efficient approximate inference methods for such crfs  usually they are sensitive to initialization and make strong assumptions  in this work  we develop an efficient  yet general algorithm for inference on fully connected crfs  the algorithm is based on a scalable sdp algorithm and the low  rank approximation of the similarity kernel matrix  the core of the proposed algorithm is a tailored quasi newton method that takes advantage of the low rank matrix approximation when solving the specialized sdp dual problem  experiments demonstrate that our method can be applied on fully connected crfs that cannot be solved previously  such as pixel level image co segmentation    p  \n",
        " p in general  self help systems are being increasingly deployed by service based industries because they are capable of delivering better customer service and increasingly the switch is to voice based self help systems because they provide a natural interface for a human to interact with a machine  a speech based self help system ideally needs a speech recognition engine to convert spoken speech to text and in addition a language processing engine to take care of any misrecognitions by the speech recognition engine  any off the shelf speech recognition engine is generally a combination of acoustic processing and speech grammar  while this is the norm  we believe that ideally a speech recognition application should have in addition to a speech recognition engine a separate language processing engine to give the system better performance  in this paper  we discuss ways in which the speech recognition engine and the language processing engine can be combined to give a better user experience    p  \n",
        " p quite recently  the algorithmic community has focused on solving multiple shortest path query problems beyond simple vertex to vertex queries  especially in the context of road networks  unfortunately  this research cannot be generalized for large scale graphs  e g   social or collaboration networks  or to efficiently answer reverse k nearest neighbor  rknn  queries  which are of practical relevance to a wide range of applications  to remedy this  we propose rehub  a novel main memory algorithm that extends the hub labeling technique to efficiently answer rknn queries on large scale networks  our experimentation will show that rehub is the best overall solution for this type of queries  requiring only minimal preprocessing and providing very fast query times    p  \n",
        " p we present an improved model and theory for time causal and time recursive spatio temporal receptive fields  obtained by a combination of gaussian receptive fields over the spatial domain and first order integrators or equivalently truncated exponential filters coupled in cascade over the temporal domain  compared to previous spatio temporal scale space formulations in terms of non enhancement of local extrema or scale invariance  these receptive fields are based on different scale space axiomatics over time by ensuring non creation of new local extrema or zero crossings with increasing temporal scale  specifically  extensions are presented about parameterizing the intermediate temporal scale levels  analysing the resulting temporal dynamics and transferring the theory to a discrete implementation in terms of recursive filters over time    p  \n",
        " p service oriented mobile social network in proximity  msnp  lets participants establish new social interactions with strangers in public proximity using heterogeneous platforms and devices  such characteristic faces challenges in discovery latency and trustworthiness  in a public service oriented msnp environment  which consists of a large number of participants  a content requester who searches for a particular service provided by other msnp participants will need to retrieve and process a large number of service description metadata  sdm  files  associated semantic metadata files and identifying the trustworthiness of the content providers  performing such tasks on a resource constraint mobile device can be time consuming  and the overall discovery performance will be affected and will result in high latency  this paper analyses the service discovery models of msnp and presents corresponding solutions to improve the service discovery performance of msnp  we firstly present and analyse the basic service discovery models of service oriented msnp  to follow up  we apply a context aware user preference prediction scheme to enhance the speed of the semantic service discovery process  later  we address the trustworthiness issue in msnp and propose a scheme to reduce the latency of the trustworthy service discovery for msnp  the proposed scheme has been tested and evaluated on msnp application prototype operating on real mobile devices and msnp simulation environments    p  \n",
        " p in a plethora of applications dealing with inverse problems  e g  in image processing  social networks  compressive sensing  biological data processing etc   the signal of interest is known to be structured in several ways at the same time  this premise has recently guided the research to the innovative and meaningful idea of imposing multiple constraints on the parameters involved in the problem under study  for instance  when dealing with problems whose parameters form sparse and low rank matrices  the adoption of suitably combined constraints imposing sparsity and low rankness  is expected to yield substantially enhanced estimation results  in this paper  we address the spectral unmixing problem in hyperspectral images  specifically  two novel unmixing algorithms are introduced  in an attempt to exploit both spatial correlation and sparse representation of pixels lying in homogeneous regions of hyperspectral images  to this end  a novel convex mixed penalty term is first defined consisting of the sum of the weighted   ell    and the weighted nuclear norm of the abundance matrix corresponding to a small area of the image determined by a sliding square window  this penalty term is then used to regularize a conventional quadratic cost function and impose simultaneously sparsity and row rankness on the abundance matrix  the resulting regularized cost function is minimized by a  an incremental proximal sparse and low rank unmixing algorithm and b  an algorithm based on the alternating minimization method of multipliers  admm   the effectiveness of the proposed algorithms is illustrated in experiments conducted both on simulated and real data    p  \n",
        " p ensuring seamless coverage accounts for the lion s share of the energy consumed in a mobile network  overlapping coverage of three to five mobile network operators  mnos  results in enormous amount of energy waste which is avoidable  the traffic demands of the mobile networks vary significantly throughout the day  as the offered load for all networks are not same at a given time and the differences in energy consumption at different loads are significant  multi mno capacity coverage sharing can dramatically reduce energy consumption of mobile networks and provide the mnos a cost effective means to cope with the exponential growth of traffic  in this paper  we propose an energy saving market for a multi mno network scenario  as the competing mnos are not comfortable with information sharing  we propose a double auction clearinghouse market mechanism where mnos sell and buy capacity in order to minimize energy consumption  in our setting  each mno proposes its bids and asks simultaneously for buying and selling multi unit capacities respectively to an independent auctioneer  i e   clearinghouse and ends up either as a buyer or as a seller in each round  we show that the mechanism allows the mnos to save significant percentage of energy cost throughout a wide range of network load  different than other energy saving features such as cell sleep or antenna muting which can not be enabled at heavy traffic load  dynamic capacity sharing allows mnos to handle traffic bursts with energy saving opportunity    p  \n",
        " p analysis of degree degree dependencies in complex networks  and their impact on processes on networks requires null models  i e  models that generate uncorrelated scale free networks  most models to date however show structural negative dependencies  caused by finite size effects  we analyze the behavior of these structural negative degree degree dependencies  using rank based correlation measures  in the directed erased configuration model  we obtain expressions for the scaling as a function of the exponents of the distributions  moreover  we show that this scaling undergoes a phase transition  where one region exhibits scaling related to the natural cut off of the network while another region has scaling similar to the structural cut off for uncorrelated networks  by establishing the speed of convergence of these structural dependencies we are able to asses statistical significance of degree degree dependencies on finite complex networks when compared to networks generated by the directed erased configuration model    p  \n",
        " p in the context of resource allocation in cloud radio access networks  recent studies assume either signal level or scheduling level coordination  this paper  instead  considers a hybrid level of coordination for the scheduling problem in the downlink of a multi cloud radio access network  as a means to benefit from both scheduling policies  consider a multi cloud radio access network  where each cloud is connected to several base stations  bss  via high capacity links  and therefore allows joint signal processing between them  across the multiple clouds  however  only scheduling level coordination is permitted  as it requires a lower level of backhaul communication  the frame structure of every bs is composed of various time frequency blocks  called power zones  pzs   and kept at fixed power level  the paper addresses the problem of maximizing a network wide utility by associating users to clouds and scheduling them to the pzs  under the practical constraints that each user is scheduled  at most  to a single cloud  but possibly to many bss within the cloud  and can be served by one or more distinct pzs within the bss  frame  the paper solves the problem using graph theory techniques by constructing the conflict graph  the scheduling problem is  then  shown to be equivalent to a maximum weight independent set problem in the constructed graph  in which each vertex symbolizes an association of cloud  user  bs and pz  with a weight representing the utility of that association  simulation results suggest that the proposed hybrid scheduling strategy provides appreciable gain as compared to the scheduling level coordinated networks  with a negligible degradation to signal level coordination    p  \n",
        " p we study  n  player turn based games played on a finite directed graph  for each play  the players have to pay a cost that they want to minimize  instead of the well known notion of nash equilibrium  ne   we focus on the notion of subgame perfect equilibrium  spe   a refinement of ne well suited in the framework of games played on graphs  we also study natural variants of spe  named weak  resp  very weak  spe  where players who deviate cannot use the full class of strategies but only a subclass with a finite number of  resp  a unique  deviation step s     p   p our results are threefold  firstly  we characterize in the form of a folk theorem the set of all plays that are the outcome of a weak spe  secondly  for the class of quantitative reachability games  we prove the existence of a finite memory spe and provide an algorithm for computing it  only existence was known with no information regarding the memory   moreover  we show that the existence of a constrained spe  i e  an spe such that each player pays a cost less than a given constant  can be decided  the proofs rely on our folk theorem for weak spes  which coincide with spes in the case of quantitative reachability games  and on the decidability of mso logic on infinite words  finally with similar techniques  we provide a second general class of games for which the existence of a  constrained  weak spe is decidable    p  \n",
        " p classifying videos according to content semantics is an important problem with a wide range of applications  in this paper  we propose a hybrid deep learning framework for video classification  which is able to model static spatial information  short term motion  as well as long term temporal clues in the videos  specifically  the spatial and the short term motion features are extracted separately by two convolutional neural networks  cnn   these two types of cnn based features are then combined in a regularized feature fusion network for classification  which is able to learn and utilize feature relationships for improved performance  in addition  long short term memory  lstm  networks are applied on top of the two features to further model longer term temporal clues  the main contribution of this work is the hybrid learning framework that can model several important aspects of the video data  we also show that     combining the spatial and the short term motion features in the regularized fusion network is better than direct classification and fusion using the cnn with a softmax layer  and     the sequence based lstm is highly complementary to the traditional classification strategy without considering the temporal frame orders  extensive experiments are conducted on two popular and challenging benchmarks  the ucf     human actions and the columbia consumer videos  ccv   on both benchmarks  our framework achieves to date the best reported performance           on the ucf     and          on the ccv    p  \n",
        " p a worldwide movement towards the publication of open government data is taking place  and budget data is one of the key elements pushing this trend  its importance is mostly related to transparency  but publishing budget data  combined with other actions  can also improve democratic participation  allow comparative analysis of governments and boost data driven business  however  the lack of standards and common evaluation criteria still hinders the development of appropriate tools and the materialization of the appointed benefits  in this paper  we present a model to analyse government initiatives to publish budget data  we identify the main features of these initiatives with a double objective   i  to drive a structured analysis  relating some dimensions to their possible impacts  and  ii  to derive characterization attributes to compare initiatives based on each dimension  we define use perspectives and analyse some initiatives using this model  we conclude that  in order to favour use perspectives  special attention must be given to user feedback  semantics standards and linking possibilities    p  \n",
        " p bidirectional recurrent neural networks  rnn  are trained to predict both in the positive and negative time directions simultaneously  they have not been used commonly in unsupervised tasks  because a probabilistic interpretation of the model is difficult  as an example of an unsupervised task  we study the problem of filling in gaps in high dimensional time series with complex dynamics  although unidirectional rnns have recently been trained successfully to model such time series  inference in the negative time direction is non trivial  we propose two probabilistic interpretations of bidirectional rnns that can be used to reconstruct missing gaps efficiently  our experiments on text data show that both proposed methods are much more accurate than unidirectional reconstructions  although a bit less accurate than a computationally complex bidirectional bayesian inference on the unidirectional rnn  we also provide results on music data for which the bayesian inference is computationally infeasible    p  \n",
        " p we give a new construction for a small space summary satisfying the coreset guarantee of a data set with respect to the  k  means objective function  the number of points required in an offline construction is in   tilde o  k  epsilon      min d k epsilon         which is minimal among all available constructions    p   p aside from two constructions with exponential dependence on the dimension  all known coresets are maintained in data streams via the merge and reduce framework  which incurs are large space dependency on   log n   instead  our construction crucially relies on johnson lindenstrauss type embeddings which combined with results from online algorithms give us a new technique for efficiently maintaining coresets in data streams without relying on merge and reduce  the final number of points stored by our algorithm in a data stream is in   tilde o  k    epsilon       log   n  min d k epsilon            p  \n",
        " p consider a bayesian problem of estimating of probability of success in a series of trials with binary outcomes  we study the asymptotic behaviour of weighted differential entropies for posterior probability density function  pdf  conditional on  x  successes after  n  trials  when  n  to  infty   in the first part of work shannon s differential entropy is considered in three particular cases   x  is a proportion of  n    x    sim n  beta   where    lt  beta lt     either  x  or  n x  is a constant  in the first and second cases limiting distribution is gaussian and the asymptotic of differential entropy is asymptotically gaussian with corresponding variances  in the third case the limiting distribution in not gaussian  but still the asymptotic of differential entropy can be found explicitly  then suppose that one is interested to know whether the coin is fair or not and for large  n  is interested in the true frequency  in other words  one wants to emphasize the parameter value  p       to do so the concept of weighted differential entropy is used when the frequency   gamma  is necessary to emphasize  it was found that the weight in suggested form does not change the asymptotic form of shannon  renyi  tsallis and fisher entropies  but change the constants  the main term in weighted fisher information is changed by some constant which depend on distance between the true frequency and the value we want to emphasize    p  \n",
        " p we consider problems of authentication using secret key generation under a privacy constraint on the enrolled source data  an adversary who has access to the stored description and correlated side information tries to deceive the authentication as well as learn about the source  we characterize the optimal tradeoff between the compression rate of the stored description  the leakage rate of the source data  and the exponent of the adversary s maximum false acceptance probability  the related problem of secret key generation with a privacy constraint is also studied where the optimal tradeoff between the compression rate  leakage rate  and secret key rate is characterized  it reveals a connection between the optimal secret key rate and security of the authentication system    p  \n",
        " p v blast detection method suffers large computational complexity due to its successive detection of symbols  in this paper  we propose a modified v blast algorithm to decrease the computational complexity by reducing the number of detection iterations required in mimo communication systems  we begin by showing the existence of a maximum number of iterations  beyond which  no significant improvement is obtained  we establish a criterion for the number of maximum effective iterations  we propose a modified algorithm that uses the measured snr to dynamically set the number of iterations to achieve an acceptable bit error rate  then  we replace the feedback algorithm with an approximate linear function to reduce the complexity  simulations show that significant reduction in computational complexity is achieved compared to the ordinary v blast  while maintaining a good ber performance    p  \n",
        " p this paper investigates an open problem introduced in       two or more mobile agents start from different nodes of a network and have to accomplish the task of gathering which consists in getting all together at the same node at the same time  an adversary chooses the initial nodes of the agents and assigns a different positive integer  called label  to each of them  initially  each agent knows its label but does not know the labels of the other agents or their positions relative to its own  agents move in synchronous rounds and can communicate with each other only when located at the same node  up to f of the agents are byzantine  a byzantine agent can choose an arbitrary port when it moves  can convey arbitrary information to other agents and can change its label in every round  in particular by forging the label of another agent or by creating a completely new one    p   p what is the minimum number m of good agents that guarantees deterministic gathering of all of them  with termination    p   p we provide exact answers to this open problem by considering the case when the agents initially know the size of the network and the case when they do not  in the former case  we prove m f   while in the latter  we prove m f    more precisely  for networks of known size  we design a deterministic algorithm gathering all good agents in any network provided that the number of good agents is at least f    for networks of unknown size  we also design a deterministic algorithm ensuring the gathering of all good agents in any network but provided that the number of good agents is at least f    both of our algorithms are optimal in terms of required number of good agents  as each of them perfectly matches the respective lower bound on m shown in       which is of f   when the size of the network is known and of f   when it is unknown    p  \n",
        " p we investigate the potential of quickest detection based on the eigenvalues of the sample covariance matrix for spectrum sensing applications  a simple phase shift keying  psk  model with additive white gaussian noise  awgn   with     primary user  pu  and  k  secondary users  sus  is considered  under both detection hypotheses   mathcal h      noise only  and   mathcal h      signal   noise  the eigenvalues of the sample covariance matrix follow wishart distributions  for the case of  k      sus  we derive an analytical formulation of the probability density function  pdf  of the maximum minimum eigenvalue  mme  detector under   mathcal h      utilizing results from the literature under   mathcal h      we investigate two detection schemes  first  we calculate the receiver operator characteristic  roc  for mme block detector based on analytical results  second  we introduce two eigenvalue based quickest detection algorithms  a cumulative sum  cusum  algorithm  when the signal to noise ratio  snr  of the pu signal is known and an algorithm using the generalized likelihood ratio  in case the snr is unknown  bounds on the mean time to false alarm   tau  text fa   and the mean time to detection   tau  text d   are given for the cusum algorithm  numerical simulations illustrate the potential advantages of the quickest detection approach over the block detection scheme    p  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " p in this paper  we propose a methodology quantifying temporal patterns of nonlinear hashtag time series  our approach is based on an analogy between neuron spikes and hashtag diffusion  we adopt the local variation  originally developed to analyze local time delays in neuron spike trains  we show that the local variation successfully characterizes nonlinear features of hashtag spike trains such as burstiness and regularity  we apply this understanding in an extreme social event and are able to observe temporal evaluation of online collective attention of twitter users to that event    p  \n",
        " p lifelogging devices are spreading faster everyday  this growth can represent great benefits to develop methods for extraction of meaningful information about the user wearing the device and his her environment  in this paper  we propose a semi supervised strategy for easily discovering objects relevant to the person wearing a first person camera  given an egocentric video sequence acquired by the camera  our algorithm uses both the appearance extracted by means of a convolutional neural network and an object refill methodology that allows to discover objects even in case of small amount of object appearance in the collection of images  a svm filtering strategy is applied to deal with the great part of the false positive object candidates found by most of the state of the art object detectors  we validate our method on a new egocentric dataset of      daily images acquired by   persons as well as on both pascal      and msrc datasets  we obtain for all of them results that largely outperform the state of the art approach  we make public both the edub dataset and the algorithm code    p  \n",
        " p scheduling wireless links for simultaneous activation in such a way that all transmissions are successfully decoded at the receivers and moreover network capacity is maximized is a computationally hard problem  usually it is tackled by heuristics whose output is a sequence of time slots in which every link appears in exactly one time slot  such approaches can be interpreted as the coloring of a graph s vertices so that every vertex gets exactly one color  here we introduce a new approach that can be viewed as assigning multiple colors to each vertex  so that  in the resulting schedule  every link may appear more than once  though the same number of times for all links   we report on extensive computational experiments  under the physical interference model  revealing substantial gains for a variety of randomly generated networks    p  \n",
        " p a function defined on the boolean hypercube is  k  fourier sparse if it has at most  k  nonzero fourier coefficients  for a function  f   mathbb f    n  rightarrow  mathbb r   and parameters  k  and  d   we prove a strong upper bound on the number of  k  fourier sparse boolean functions that disagree with  f  on at most  d  inputs  our bound implies that the number of uniform and independent random samples needed for learning the class of  k  fourier sparse boolean functions on  n  variables exactly is at most  o n  cdot k  log k      p   p as an application  we prove an upper bound on the query complexity of testing booleanity of fourier sparse functions  our bound is tight up to a logarithmic factor and quadratically improves on a result due to gur and tamuz  chicago j  theor  comput  sci            p  \n",
        " p all modern processors include a set of vector instructions  while this gives a tremendous boost to the performance  it requires a vectorized code that can take advantage of such instructions  as an ideal vectorization is hard to achieve in practice  one has to decide when different instructions may be applied to different elements of the vector operand  this is especially important in implicit vectorization as in nvidia cuda single instruction multiple threads  simt  model  where the vectorization details are hidden from the programmer  in order to assess the costs incurred by incompletely vectorized code  we have developed a micro benchmark that measures the characteristics of the cuda thread divergence model on different architectures focusing on the loops performance    p  \n",
        " p we exhibit families of     cnf formulas over  n  variables that have sums of squares  sos  proofs of unsatisfiability of degree  a k a  rank   d  but require sos proofs of size  n   omega d    for values of  d   d n   from constant all the way up to  n   delta   for some universal constant  delta   this shows that the  n  o d    running time obtained by using the lasserre semidefinite programming relaxations to find degree  d  sos proofs is optimal up to constant factors in the exponent  we establish this result by combining   mathsf np   reductions expressible as low degree sos derivations with the idea of relativizing cnf formulas in  kraj  i v c ek      and  dantchev and riis      and then applying a restriction argument as in  atserias  m  uller  and oliva      and  atserias  lauria  and nordstr  om       this yields a generic method of amplifying sos degree lower bounds to size lower bounds  and also generalizes the approach in  aln    to obtain size lower bounds for the proof systems resolution  polynomial calculus  and sherali adams from lower bounds on width  degree  and rank  respectively    p  \n",
        " p in this paper  we demonstrate that when the ratio  n  of the number of antenna elements  n  to the number  p  of radiating sources is superior or equal to      then it is possible to choose a propagator from a set of  n n         operators to compute the angles of arrival  aoa  of the narrowband incoming waves  this new non eigenbased approach is efficient when the signal to noise ratio  snr  is moderate  and gives multitude of possibilities  that are dependent of the random data  to construct the complex sets whose columns are orthogonal to the signal subspace generated by the radiating sources  elementary examples are given for  n      n    and  n     the simulation results are presented to illustrate the performance of the proposed computational methods    p  \n",
        " p the direction of arrival  doa  estimation problem involves the localization of a few sources from a limited number of observations on an array of sensors  thus it can be formulated as a sparse signal reconstruction problem and solved efficiently with compressive sensing  cs  to achieve high resolution imaging  on a discrete angular grid  the cs reconstruction degrades due to basis mismatch when the doas do not coincide with the angular directions on the grid  to overcome this limitation  a continuous formulation of the doa problem is employed and an optimization procedure is introduced  which promotes sparsity on a continuous optimization variable  the doa estimation problem with infinitely many unknowns  i e   source locations and amplitudes  is solved over a few optimization variables with semidefinite programming  the grid free cs reconstruction provides high resolution imaging even with non uniform arrays  single snapshot data and under noisy conditions as demonstrated on experimental towed array data    p  \n",
        " p a nationally representative study of video game play among adolescents in the united states showed that     of adolescents aged    to    years play computer  web  and portable  or console  video games  lenhart et al          we hypothesized that if people play games as a regular exercise regime  gaming will correlate with an improvement in their cognitive skills  for this experiment  a few games that tested the logical reasoning and critical analysis skills under a given time constraint were coded in python using pygame and were played by a group of  th grade students  in order to test whether there is a relationship between gaming and test performance  we divided the students into two groups and gave them tests before and after the experimentation period in order to measure their improvement  one group played the games while the other did not  in the group of students that played the games  an average improvement of        was seen  p  lt           the group that did not play the games only improved their performance by an average of         p              p  \n",
        " p storage devices based on flash memory have replaced hard disk drives  hdds  due to their superior performance  increasing density  and lower power consumption  unfortunately  flash memory is subject to challenging idiosyncrasies like erase before write and limited block lifetime  these constraints are handled by a flash translation layer  ftl   which performs out of place updates  wear leveling and garbage collection behind the scene  while offering the application a virtualization of the physical address space    p   p a class of relevant ftls employ a flash resident page associative mapping table from logical to physical addresses  with a smaller ram resident cache for frequently mapped entries  in this paper  we address the problem of performing garbage collection under such ftls  we observe two problems  firstly  maintaining the metadata needed to perform garbage collection under these schemes is problematic  because at write time we do not necessarily know the physical address of the before image  secondly  the size of this metadata must remain small  because it makes ram unavailable for caching frequently accessed entries  we propose two complementary techniques  called lazy gecko and logarithmic gecko  which address these issues  lazy gecko works well when ram is plentiful enough to store the gc metadata  logarithmic gecko works well when ram isn t plentiful and efficiently stores the gc metadata in flash  thus  these techniques are applicable to a wide range of flash devices with varying amounts of embedded ram    p  \n",
        " p this paper contributes a jointly embedding model for predicting relations between a pair of entities in the scenario of knowledge population  it differs from most stand alone approaches which separately perform on either knowledge bases or free texts  the proposed model simultaneously learns low dimensional vector representations for both triplets in knowledge repositories and the mentions of relations in free texts  so that we can leverage the evidences from both of the two resources to make more accurate predictions  we use nell to evaluate the performance of our approach  compared with most of cutting edge methods  results of extensive experiments show that our model achieves significant improvement on relation extraction    p  \n",
        " p traditional way of storing facts in triplets    it head  entity  relation  tail  entity    abbreviated as    it h  r  t    makes the knowledge intuitively displayed and easily acquired by mankind  but hardly computed or even reasoned by ai machines  inspired by the success in applying   it distributed representations  to ai related fields  recent studies expect to represent each entity and relation with a unique low dimensional embedding  which is different from the symbolic and atomic framework of displaying knowledge in triplets  in this way  the knowledge computing and reasoning can be essentially facilitated by means of a simple   it vector calculation   i e     bf h      bf r   approx   bf t    we thus contribute an effective model to learn better embeddings satisfying the formula by pulling the positive tail entities    bf t       to get together and close to   bf h      bf r     it nearest neighbor    and simultaneously pushing the negatives    bf t       away from the positives    bf t       via keeping a   it large margin   we also design a corresponding learning algorithm to efficiently find the optimal solution based on   it stochastic gradient descent  in iterative fashion  quantitative experiments illustrate that our approach can achieve the state of the art performance  compared with several latest methods on some benchmark datasets for two classical applications  i e    it link prediction  and   it triplet classification   moreover  we analyze the parameter complexities among all the evaluated models  and analytical results indicate that our model needs fewer computational resources on outperforming the other methods    p  \n",
        " p the compute and forward framework permits each receiver in a gaussian network to directly decode a linear combination of the transmitted messages  the resulting linear combinations can then be employed as an end to end communication strategy for relaying  interference alignment  and other applications  recent efforts have demonstrated the advantages of employing unequal powers at the transmitters and decoding more than one linear combination at each receiver  however  neither of these techniques fit naturally within the original formulation of compute and forward  this paper proposes an expanded compute and forward that incorporates both of these possibilities and permits an intuitive interpretation in terms of signal levels  within this framework  recent achievability and optimality results are unified and generalized    p  \n",
        " p this paper presents a demo of our security toolbox to detect novel malware in android apps  this toolbox is developed through our recent research project funded by the darpa automated program analysis for cybersecurity  apac  project  the adversarial challenge   red   teams in the darpa apac program are tasked with designing sophisticated malware to test the bounds of malware detection technology being developed by the research and development   blue   teams  our research group  a blue team in the darpa apac program  proposed a  human in the loop program analysis  approach to detect malware given the source or java bytecode for an android app  our malware detection apparatus consists of two components  a general purpose program analysis platform called atlas  and a security toolbox built on the atlas platform  this paper describes the major design goals  the toolbox components to achieve the goals  and the workflow for auditing android apps  the accompanying video   a href  http   youtu be whcoax hinu  this http url  a   illustrates features of the toolbox through a live audit    p  \n",
        " p recent years have demonstrated that using random feature maps can significantly decrease the training and testing times of kernel based algorithms without significantly lowering their accuracy  regrettably  because random features are target agnostic  typically thousands of such features are necessary to achieve acceptable accuracies  in this work  we consider the problem of learning a small number of explicit polynomial features  our approach  named tensor machines  finds a parsimonious set of features by optimizing over the hypothesis class introduced by kar and karnick for random feature maps in a target specific manner  exploiting a natural connection between polynomials and tensors  we provide bounds on the generalization error of tensor machines  empirically  tensor machines behave favorably on several real world datasets compared to other state of the art techniques for learning polynomial features  and deliver significantly more parsimonious models    p  \n",
        " p we consider the recovery of sparse signals that share a common support from multiple measurement vectors  the performance of several algorithms developed for this task depends on parameters like dimension of the sparse signal  dimension of measurement vector  sparsity level  measurement noise  we propose a fusion framework  where several multiple measurement vector reconstruction algorithms participate and the final signal estimate is obtained by combining the signal estimates of the participating algorithms  we present the conditions for achieving a better reconstruction performance than the participating algorithms  numerical simulations demonstrate that the proposed fusion algorithm often performs better than the participating algorithms    p  \n",
        " p two player  zero sum games of infinite duration and their quantitative versions are often used in verification to model the interaction between a controller  eve  and an antagonistic environment  adam   the question usually addressed is that of the existence  and computability  of a strategy for eve that can maximize her payoff against any strategy of adam  in this work we are interested in strategies of eve that minimize her regret  that is to say  we are interested in finding a strategy that minimizes the difference between her actual payoff and the payoff she could have achieved if she had known the strategy of adam in advance  we give algorithms to compute the strategies of eve that ensure minimal regret against an adversary whose choice of strategy is  i  unrestricted   ii  limited to positional strategies  or  iii  limited to word strategies  we also establish relations between the latter version and other problems studied in the literature    p  \n",
        " p cost register automata  cra  were proposed by alur et all as an alternative model for weighted automata  in hope of finding decidable subclasses of cra  they proposed to restrict their model with the copyless restriction but nothing is really know about the structure or properties of this new computational model called copyless cra    p   p in this paper we study the properties and expressiveness of copyless cra  we propose a normal form for copyless cra and we study the properties of a special group of registers  called stable registers   furthermore  we find that copyless cra do not have good closure properties since we show that they are not closed under reverse operation  finally  we propose a subclass of copyless cra and we show that this subclass is closed under regular lookahead    p  \n",
        " p numerous groups have applied a variety of deep learning techniques to computer vision problems in highway perception scenarios  in this paper  we presented a number of empirical evaluations of recent deep learning advances  computer vision  combined with deep learning  has the potential to bring about a relatively inexpensive  robust solution to autonomous driving  to prepare deep learning for industry uptake and practical applications  neural networks will require large data sets that represent all possible driving environments and scenarios  we collect a large data set of highway data and apply deep learning and computer vision algorithms to problems such as car and lane detection  we show how existing convolutional neural networks  cnns  can be used to perform lane and vehicle detection while running at frame rates required for a real time system  our results lend credence to the hypothesis that deep learning holds promise for autonomous driving    p  \n",
        " p dedicated systems are fundamental for neuroscience experimental protocols that require timing determinism and synchronous stimuli generation  we developed a data acquisition and stimuli generator system for neuroscience research  optimized for recording timestamps from up to   spiking neurons and entirely specified in a high level hardware description language  hdl   despite the logic complexity penalty of synthesizing from such a language  it was possible to implement our design in a low cost small reconfigurable device  under a modular framework  we explored two different memory arbitration schemes for our system  evaluating both their logic element usage and resilience to input activity bursts  one of them was designed with a decoupled and latency insensitive approach  allowing for easier code reuse  while the other adopted a centralized scheme  constructed specifically for our application  the usage of a high level hdl allowed straightforward and stepwise code modifications to transform one architecture into the other  the achieved modularity is very useful for rapidly prototyping novel electronic instrumentation systems tailored to scientific research    p  \n",
        " p there are many resources useful for processing images  most of them freely available and quite friendly to use  in spite of this abundance of tools  a study of the processing methods is still worthy of efforts  here  we want to discuss the possibilities arising from the use of fractional differential calculus  this calculus evolved in the research field of pure mathematics until       when applied science started to use it  only recently  fractional calculus was involved in image processing methods  as we shall see  the fractional calculation is able to enhance the quality of images  with interesting possibilities in edge detection and image restoration  we suggest also the fractional differentiation as a tool to reveal faint objects in astronomical images    p  \n",
        " p a receiver wants to compute a function of two correlated sources separately observed by two transmitters  one of the transmitters may send a possibly private message to the other transmitter in a cooperation phase before both transmitters communicate to the receiver  for this network configuration this paper investigates both a function computation setup  wherein the receiver wants to compute a given function of the sources exactly  and a rate distortion setup  wherein the receiver wants to compute a given function within some distortion    p   p for the function computation setup  a general inner bound to the rate region is established and shown to be tight in a number of cases  partially invertible functions  full cooperation between transmitters  one round point to point communication  two round point to point communication  and the cascade setup where the transmitters and the receiver are aligned  in particular it is shown that the ratio of the total number of transmitted bits without cooperation and the total number of transmitted bits with cooperation can be arbitrarily large  furthermore  one bit of cooperation suffices to arbitrarily reduce the amount of information both transmitters need to convey to the receiver    p   p for the rate distortion version  an inner bound to the rate region is exhibited which always includes  and sometimes strictly  the convex hull of kaspi berger s related inner bounds  the strict inclusion is shown via two examples    p  \n",
        " p this paper establishes the rate region for a class of source coding function computation setups where sources of information are available at the nodes of a tree and where a function of these sources must be computed at the root  the rate region holds for any function as long as the sources  joint distribution satisfies a certain markov criterion  this criterion is met  in particular  when the sources are independent    p   p this result recovers the rate regions of several function computation setups  these include the point to point communication setting with arbitrary sources  the noiseless multiple access network with  conditionally independent sources   and the cascade network with markovian sources    p  \n",
        " p in this paper  we discuss the potential for improvement of the simple random access scheme by utilizing local information such as the received signal to interference plus noise ratio  sinr   we propose a spatially adaptive random access  sara  scheme in which the transmitters in the network utilize different transmit probabilities depending on the local situation  in our proposed scheme  the transmit probability is adaptively updated by the ratio of the received sinr and the target sinr  we investigate the performance of the spatially adaptive random access scheme  for the comparison  we derive an optimal transmit probability of aloha random access scheme in which all transmitters use the same transmit probability  we illustrate the performance of the spatially adaptive random access scheme through simulations  we show that the performance of the proposed scheme surpasses that of the optimal aloha random access scheme and is comparable with the csma ca scheme  the convergence property of the proposed scheme is analyzed using the standard interference function    p  \n",
        " p we study the asymmetric binary matrix partition problem that was recently introduced by alon et al   wine       to model the impact of asymmetric information on the revenue of the seller in take it or leave it sales  instances of the problem consist of an  n  times m  binary matrix  a  and a probability distribution over its columns  a partition scheme  b  b       b n   consists of a partition  b i  for each row  i  of  a   the partition  b i  acts as a smoothing operator on row  i  that distributes the expected value of each partition subset proportionally to all its entries  given a scheme  b  that induces a smooth matrix  a b   the partition value is the expected maximum column entry of  a b   the objective is to find a partition scheme such that the resulting partition value is maximized  we present a        approximation algorithm for the case where the probability distribution is uniform and a       e   approximation algorithm for non uniform distributions  significantly improving results of alon et al  although our first algorithm is combinatorial  and very simple   the analysis is based on linear programming and duality arguments  in our second result we exploit a nice relation of the problem to submodular welfare maximization    p  \n",
        " p how much cutting is needed to simplify the topology of a surface  we provide bounds for several instances of this question  for the minimum length of topologically non trivial closed curves  pants decompositions  and cut graphs with a given combinatorial map in triangulated combinatorial surfaces  or their dual cross metric counterpart     p   p our work builds upon riemannian systolic inequalities  which bound the minimum length of non trivial closed curves in terms of the genus and the area of the surface  we first describe a systematic way to translate riemannian systolic inequalities to a discrete setting  and vice versa  this implies a conjecture by przytycka and przytycki from       a number of new systolic inequalities in the discrete setting  and the fact that a theorem of hutchinson on the edge width of triangulated surfaces and gromov s systolic inequality for surfaces are essentially equivalent  we also discuss how these proofs generalize to higher dimensions    p   p then we focus on topological decompositions of surfaces  relying on ideas of buser  we prove the existence of pants decompositions of length o g      n        for any triangulated combinatorial surface of genus g with n triangles  and describe an o gn  time algorithm to compute such a decomposition    p   p finally  we consider the problem of embedding a cut graph  or more generally a cellular graph  with a given combinatorial map on a given surface  using random triangulations  we prove  essentially  that  for any choice of a combinatorial map  there are some surfaces on which any cellular embedding with that combinatorial map has length superlinear in the number of triangles of the triangulated combinatorial surface  there is also a similar result for graphs embedded on polyhedral triangulations    p  \n",
        " p assuring safety in discrete time stochastic hybrid systems is particularly difficult when only partial observations are available  we first review a formulation of the probabilistic viability  i e  safety  problem under noisy hybrid observations as a dynamic program  two methods for approximately solving the dynamic program are presented  the first method approximates the hybrid system as an equivalent finite state markov decision process  so that the information state is a probability mass function  the second approach approximates an indicator function over the safe region using radial basis functions  to represent the information state as a gaussian mixture  in both cases  we discretize the hybrid observation process and generate a sampled set of information states  then use point based value iteration to under approximate the viability probability  we obtain error bounds and convergence results in both cases  assuming additive gaussian noise in the continuous state dynamics and observations  we compare the performance of the finite state and gaussian mixture approaches on a simple numerical example    p  \n",
        " p given a real valued function  f  defined over a manifold  m  embedded in   mathbb r  d   we are interested in recovering structural information about  f  from the sole information of its values on a finite sample  p   existing methods provide approximation to the persistence diagram of  f  when geometric noise and functional noise are bounded  however  they fail in the presence of aberrant values  also called outliers  both in theory and practice    p   p we propose a new algorithm that deals with outliers  we handle aberrant functional values with a method inspired from the k nearest neighbors regression and the local median filtering  while the geometric outliers are handled using the distance to a measure  combined with topological results on nested filtrations  our algorithm performs robust topological analysis of scalar fields in a wider range of noise models than handled by current methods  we provide theoretical guarantees and experimental results on the quality of our approximation of the sampled scalar field    p  \n",
        " p in this report  we describe a theano based alexnet  krizhevsky et al         implementation and its naive data parallelism on multiple gpus  our performance on   gpus is comparable with the state of art caffe library  jia et al         run on   gpu  to the best of our knowledge  this is the first open source python based alexnet implementation to date    p  \n",
        " p although stochastic approximation learning methods have been widely used in the machine learning literature for over    years  formal theoretical analyses of specific machine learning algorithms are less common because stochastic approximation theorems typically possess assumptions which are difficult to communicate and verify  this paper presents a new stochastic approximation theorem for state dependent noise with easily verifiable assumptions applicable to the analysis and design of important deep learning algorithms including  adaptive learning  contrastive divergence learning  stochastic descent expectation maximization  and active learning    p  \n",
        " p finding minima of a real valued non convex function over a high dimensional space is a major challenge in science  we provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre image contains the bulk of its critical points  this is in contrast with the low dimensional picture in which this band is wide  our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity  furthermore our experiments on teacher student networks with the mnist dataset establish a similar phenomenon in deep networks  we finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps    p  \n",
        " p scientometrics is the study of the quantitative aspects of the process of science as a communication system  it is centrally  but not only  concerned with the analysis of citations in the academic literature  in recent years it has come to play a major role in the measurement and evaluation of research performance  in this review we consider  the historical development of scientometrics  sources of citation data  citation metrics and the  laws  of scientometrics  normalisation  journal impact factors and other journal metrics  visualising and mapping science  evaluation and policy  and future developments    p  \n",
        " p this paper investigates stochastic and adversarial combinatorial multi armed bandit problems  in the stochastic setting  we first derive problem specific regret lower bounds  and analyze how these bounds scale with the dimension of the decision space  we then propose combucb  algorithms that efficiently exploit the combinatorial structure of the problem  and derive finite time upper bound on their regrets  these bounds improve over regret upper bounds of existing algorithms  and we show numerically that combucb significantly outperforms any other algorithm  in the adversarial setting  we propose two simple algorithms  namely combexp   and combexp   for semi bandit and bandit feedback  respectively  their regrets have similar scaling as state of the art algorithms  in spite of the simplicity of their implementation    p  \n",
        " p in this paper  the capacity region of the two user linear deterministic  ld  interference channel with noisy output feedback  ic nof  is fully characterized  this result allows the identification of several asymmetric scenarios in which imple  menting channel output feedback in only one of the transmitter  receiver pairs is as beneficial as implementing it in both links  in terms of achievable individual rate and sum rate improvements w r t  the case without feedback  in other scenarios  the use of channel output feedback in any of the transmitter receiver pairs benefits only one of the two pairs in terms of achievable individual rate improvements or simply  it turns out to be useless  i e   the capacity regions with and without feedback turn out to be identical even in the full absence of noise in the feedback links    p  \n",
        " p metric data structures  distance oracles  distance labeling schemes  routing schemes  and low distortion embeddings provide a powerful algorithmic methodology  which has been successfully applied for approximation algorithms  cite llr   online algorithms  cite bbmn     distributed algorithms  cite kkmpt    and for computing sparsifiers  cite st     however  this methodology appears to have a limitation  the worst case performance inherently depends on the cardinality of the metric  and one could not specify in advance which vertices points should enjoy a better service  i e   stretch distortion  label size dimension  than that given by the worst case guarantee    p   p in this paper we alleviate this limitation by devising a suit of   em prioritized  metric data structures and embeddings  we show that given a priority ranking   x   x    ldots x n   of the graph vertices  respectively  metric points  one can devise a metric data structure  respectively  embedding  in which the stretch  resp   distortion  incurred by any pair containing a vertex  x j  will depend on the rank  j  of the vertex  we also show that other important parameters  such as the label size and  in some sense  the dimension  may depend only on  j   in some of our metric data structures  resp   embeddings  we achieve both prioritized stretch  resp   distortion  and label size  resp   dimension    em simultaneously   the worst case performance of our metric data structures and embeddings is typically asymptotically no worse than of their non prioritized counterparts    p  \n",
        " p our overall goal is to unify and extend some results in the literature related to the approximation of generating functions of finite and infinite sequences over a field by rational functions  in our approach  numerators play a significant role  we revisit a theorem of niederreiter on  i  linear complexities and  ii    n  th   minimal polynomials  of an infinite sequence  proved using partial quotients  we prove  i  and its converse from first principles and generalise  ii  to rational functions where the denominator need not have minimal degree  we prove  ii  in two parts  firstly for geometric sequences and then for sequences with a jump in linear complexity  the basic idea is to decompose the denominator as a sum of polynomial multiples of two polynomials of minimal degree  there is a similar decomposition for the numerators  the decomposition is unique when the denominator has degree at most the length of the sequence  the proof also applies to rational functions related to finite sequences  generalising a result of massey  we give a number of applications to rational functions associated to sequences    p  \n",
        " p we provide a general framework to remove short advice by formulating the following computational task for a function  f   given two oracles at least one of which is honest  i e  correctly computes  f  on all inputs  as well as an input  the task is to compute  f  on the input with the help of the oracles by a probabilistic polynomial time machine  which we shall call a selector  we characterize the languages for which short advice can be removed by the notion of selector  a paddable language has a selector if and only if short advice of a probabilistic machine that accepts the language can be removed under any relativized world  previously  instance checkers have served as a useful tool to remove short advice of probabilistic computation  we indicate that existence of instance checkers is a property stronger than that of removing short advice  although no instance checker for    rm exp    rm np   complete languages exists unless    rm exp    rm np      rm nexp    we prove that there exists a selector for any    rm exp    rm np   complete language  by building on the proof of    rm mip      rm nexp   by babai  fortnow  and lund           p  \n",
        " p matrix sketching schemes and the nystr  om method have both been extensively used to speed up large scale eigenvalue computation and kernel learning methods  matrix sketching methods produce accurate matrix approximations  but they are only computationally efficient on skinny matrices where one of the matrix dimensions is relatively small  in particular  they are not efficient on large square matrices  the nystr  om method  on the other hand  is highly efficient on symmetric  and thus square  matrices  but can only achieve low matrix approximation accuracy  in this paper we propose a novel combination of the sketching method and the nystr  om method to improve their efficiency effectiveness  leading to a novel approximation which we call the sketch nystr  om method  the sketch nystr  om method is computationally nearly as efficient as the nystr  om method on symmetric matrices with approximation accuracy comparable to that of the sketching method  we show theoretically that the sketch nystr  om method can potentially solve eigenvalue problems and kernel learning problems in linear time with respect to the matrix size to achieve     epsilon  relative error  whereas the sketch methods and the nystr  om method cost at least quadratic time to attain comparable error bound  our technique can be straightforwardly applied to make the cur matrix decomposition more efficiently computed without much affecting the accuracy  empirical experiments demonstrate the effectiveness of the proposed methods    p  \n",
        " p traditional disease surveillance systems suffer from several disadvantages  including reporting lags and antiquated technology  that have caused a movement towards internet based disease surveillance systems  internet systems are particularly attractive for disease outbreaks because they can provide data in near real time and can be verified by individuals around the globe  however  most existing systems have focused on disease monitoring and do not provide a data repository for policy makers or researchers  in order to fill this gap  we analyzed wikipedia article content    p   p we demonstrate how a named entity recognizer can be trained to tag case counts  death counts  and hospitalization counts in the article narrative that achieves an f  score of        we also show  using the the      west african ebola virus disease epidemic article as a case study  that there are detailed time series data that are consistently updated that closely align with ground truth data    p   p we argue that wikipedia can be used to create the first community driven open source emerging disease detection  monitoring  and repository system    p  \n",
        " p this paper proposes a new convolutional neural architecture based on tree structures  called the tree based convolutional neural network  tbcnn   two variants take advantage of constituency trees and dependency trees  respectively  to model sentences  compared with traditional  flat  convolutional neural networks  cnns   tbcnns explore explicitly the structural information of sentences  compared with recursive neural networks  tbcnns have much shorter propagation paths  enabling more effective feature learning and extraction  in the experiment of sentiment analysis  our two models consistently outperform cnns and rnns in a controlled setting  our models are also competitive to most state of the art results  including rnns based on long short term memory and deep cnns rnns    p  \n",
        " p we study the multi broadcast problem in multi hop wireless networks under the sinr model deployed in the  d euclidean plane  in multi broadcast  there are  k  initial rumours  potentially belonging to different nodes  that must be forwarded to all  n  nodes of the network  furthermore  in each round a node can only transmit a small message that could contain at most one initial rumor and  o  log n   control bits  in order to be successfully delivered to a node  transmissions must satisfy the  signal to inference and noise ratio  sinr condition and have sufficiently strong signal at the receiver  we present deterministic algorithms for multi broadcast for different settings that reflect the different types of knowledge about the topology of the network available to the nodes   i  the whole network topology  ii  their own coordinates and coordinates of their neighbors  iii  only their own coordinates  and  iv  only their own ids and the ids of their neighbors  for the former two settings  we present solutions that are scalable with respect to the diameter of the network and the polylogarithm of the network size  i e     log c n  for some constant  c gt      while the solutions for the latter two have round complexity that is superlinear in the number of nodes  the last result is of special significance  as it is the first result for the sinr model that does not require nodes to know their coordinates in the plane  a very specialized type of knowledge   but intricately exploits the understanding that nodes are implanted in the  d euclidean plane    p  \n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import csv\n",
      "import scipy.sparse\n",
      "import numpy as np\n",
      "\n",
      "class Parser:\n",
      "    @staticmethod\n",
      "    def load_matrix(matrix_file, offset=0):\n",
      "        cr = csv.reader(open(matrix_file))\n",
      "        rows=[]\n",
      "        cols=[]\n",
      "        entries=[]\n",
      "        for triplet in cr:\n",
      "            rows.append(int(triplet[0])+offset)\n",
      "            cols.append(int(triplet[1])+offset)\n",
      "            entries.append(int(triplet[2]))\n",
      "        rows = np.array(rows)\n",
      "        cols = np.array(cols)\n",
      "        entries = np.array(entries)\n",
      "\n",
      "        return scipy.sparse.csc_matrix((entries,(rows,cols)))\n",
      "\n",
      "    @staticmethod\n",
      "    def load_dict(dict_file):\n",
      "        cr = csv.reader(open(dict_file))\n",
      "        words=[]\n",
      "        for word_count_pair in cr:\n",
      "            words.append(word_count_pair[0])\n",
      "        return words\n",
      "    @staticmethod\n",
      "    def load_title(title_file):\n",
      "        f=open(title_file)\n",
      "        titles=[]\n",
      "        titles = f.readlines()\n",
      "        titles = [line.strip() for line in titles]\n",
      "        return titles\n",
      "    @staticmethod\n",
      "    def load_category(category_file):\n",
      "        cr = csv.reader(open(category_file))\n",
      "        categories=[]\n",
      "        for category_count_pair in cr:\n",
      "            categories.append(category_count_pair[0])\n",
      "        return categories\n",
      "\n",
      "class Options:\n",
      "    def __init__(self, threshold_m, threshold_n, max_iteration, tolerance, num_pc):\n",
      "        self.threshold_m = threshold_m\n",
      "        self.threshold_n = threshold_n\n",
      "        self.max_iteration = max_iteration\n",
      "        self.tolerance = tolerance\n",
      "        self.num_pc = num_pc\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.sparse\n",
      "import math\n",
      "import numpy\n",
      "\n",
      "class Iterator:\n",
      "    def __init__(self, options, M):\n",
      "        self.M = scipy.sparse.csc_matrix(M)\n",
      "        self.options = options\n",
      "    def thresh(self, s_vector, num_entries):\n",
      "        listOfValues = s_vector.T.todense().tolist()[0]\n",
      "        indices = self.sort(listOfValues)[1]\n",
      "        thresholded_vec = numpy.zeros(shape=s_vector.T.shape)\n",
      "        for i in range(num_entries):\n",
      "            index = indices[i]\n",
      "            thresholded_vec[0, index] = listOfValues[index]\n",
      "        return scipy.sparse.csr_matrix(thresholded_vec).T\n",
      "\n",
      "    def top_k_ind(self, s_vector, num_entries):\n",
      "        listOfValues = s_vector.T.todense().tolist()[0]\n",
      "        return self.sort(listOfValues)\n",
      "\n",
      "    def sort(self, vector):\n",
      "        indices = range(len(vector))\n",
      "        val_ind = map (lambda x : (abs(vector[x]),x), indices)\n",
      "        sorted_val_ind = sorted(val_ind)\n",
      "        sorted_val_ind.reverse()\n",
      "        sorted_indices = map (lambda x : x[1], sorted_val_ind)\n",
      "        sorted_values = map (lambda x : x[0], sorted_val_ind)\n",
      "        return (sorted_values, sorted_indices)\n",
      "    def single_iteration (self):\n",
      "\n",
      "        M = self.M\n",
      "        m = self.M.shape[0]\n",
      "        n = self.M.shape[1]\n",
      "        k_p = self.options.threshold_m\n",
      "        k_q = self.options.threshold_n\n",
      "        tolerance = self.options.tolerance\n",
      "        max_iteration = self.options.max_iteration\n",
      "\n",
      "\n",
      "        ini = self.ini_mat();\n",
      "        p = ini[0]\n",
      "        q = ini[1]\n",
      "        obj0 = float(\"inf\")\n",
      "        converged = False\n",
      "        iter = 0\n",
      "        while not converged:\n",
      "            p_new = self.thresh((M*q), k_p)\n",
      "            #print 'value is ' + str(sum(map(lambda x: x*x, p_new.T.todense().tolist()[0])))\n",
      "            #print 'max p is' + str(max(p_new.todense().tolist()[0]))\n",
      "            p = p_new\n",
      "            #print 'value is ' + str(p_new.todense())\n",
      "            #print 'q thresholding: '\n",
      "            q_new = self.thresh((p.T * M).T, k_q)\n",
      "            q = q_new/sum(map(abs,(q_new.data)))\n",
      "            q_test = (p.T * M)\n",
      "            #print 'max qt is' + str((p.T * M).T.shape)\n",
      "            #print 'max q is' + str(max(q_new.todense().tolist()[0]))\n",
      "            updated_mat = M-p_new*(q_new.T)\n",
      "            obj1 = updated_mat.data.dot(updated_mat.data)\n",
      "            if (abs(obj1-obj0) <= tolerance or iter >= max_iteration):\n",
      "                converged = 1\n",
      "            iter = iter + 1\n",
      "            obj0 = obj1\n",
      "        return (p,q)\n",
      "\n",
      "    def multiple_iterations (self):\n",
      "        term_inds = []\n",
      "        doc_inds = []\n",
      "        term_vals = []\n",
      "        doc_vals = []\n",
      "        for i in range(self.options.num_pc):\n",
      "            print 'handling pc# ' + str(i)\n",
      "            (p,q) = self.single_iteration()\n",
      "            (p_val, p_ind) = self.top_k_ind(p, self.options.threshold_m)\n",
      "            (q_val, q_ind) = self.top_k_ind(q, self.options.threshold_n)\n",
      "            doc_inds.append(p_ind[0:self.options.threshold_m])\n",
      "            term_inds.append(q_ind[0:self.options.threshold_n])\n",
      "            doc_vals.append(p_val[0:self.options.threshold_m])\n",
      "            term_vals.append(q_val[0:self.options.threshold_n])\n",
      "            self.remove_cols(q_ind[0:self.options.threshold_n])\n",
      "            self.remove_rows(p_ind[0:self.options.threshold_m])\n",
      "\n",
      "        return ((doc_vals,doc_inds),(term_vals,term_inds))\n",
      "\n",
      "    @staticmethod\n",
      "    def run(matrix_path, dict_path, title_path, category_path):\n",
      "        print 'running'\n",
      "        matrix = Parser.load_matrix(matrix_path,-1)\n",
      "        dict = Parser.load_dict(dict_path)\n",
      "        titlelist = Parser.load_title(title_path)\n",
      "\n",
      "        categorylist = Parser.load_title(category_path)\n",
      "        opt = Options(5, 5, 50, 0.0001, 3)\n",
      "        it = Iterator(opt, matrix)\n",
      "        word_pcs = []\n",
      "        title_pcs = []\n",
      "\n",
      "        dwords='var dwords=['\n",
      "        fnames = 'var fnames=['\n",
      "        links = 'var links=['\n",
      "        values = 'var values=['\n",
      "        fvalues = 'var fvalues=['\n",
      "\n",
      "        ((p_vals,p_inds),(q_vals,q_inds)) = it.multiple_iterations()\n",
      "        outputfile = open('output.txt','w')\n",
      "        for pc, pcv in zip(q_inds, q_vals):\n",
      "            word_pc = []\n",
      "            dwordstr = '['\n",
      "            valstr = '['\n",
      "            for q_ind, q_val in zip(pc, pcv):\n",
      "                word_pc.append('%.4f\\t%s' % (q_val, dict[q_ind]))\n",
      "                dwordstr = dwordstr + format('\"%s\",' %(dict[q_ind]))\n",
      "                valstr = valstr + format('%.3f,' %(q_val))\n",
      "            dwordstr = dwordstr[0:len(dwordstr)-1] + ']'\n",
      "            valstr = valstr[0:len(valstr)-1] + ']'\n",
      "            dwords = dwords+dwordstr+','\n",
      "            values = values + valstr+','\n",
      "\n",
      "            word_pcs.append(word_pc)\n",
      "            todel = sorted(pc);\n",
      "            todel.reverse();\n",
      "            for q_ind in todel:\n",
      "                del dict[q_ind]\n",
      "        for pc, pcv in zip(p_inds, p_vals):\n",
      "            title_pc = []\n",
      "            fnamestr = '['\n",
      "            fvalstr = '['\n",
      "            linkstr = '['\n",
      "            for p_ind,p_val in zip(pc,pcv):\n",
      "                title_pc.append('%.4f\\t%s\\t%s' % (p_val,titlelist[p_ind],categorylist[p_ind]))\n",
      "                fnamestr = fnamestr + format('\"%s\",' %(titlelist[p_ind]))\n",
      "                linkstr = linkstr + format('\"%s\",' %(categorylist[p_ind]))\n",
      "                fvalstr = fvalstr + format('%.3f,' %(p_val))\n",
      "            fnamestr = fnamestr[0:len(fnamestr)-1] + ']'\n",
      "            fvalstr = fvalstr[0:len(fvalstr)-1] + ']'\n",
      "            linkstr = linkstr[0:len(linkstr)-1] + ']'\n",
      "\n",
      "            fnames = fnames + fnamestr+','\n",
      "            fvalues = fvalues + fvalstr + ','\n",
      "            links = links + linkstr + ',';\n",
      "            title_pcs.append(title_pc)\n",
      "            todel = sorted(pc);\n",
      "            todel.reverse();\n",
      "            for p_ind in todel:\n",
      "                del titlelist[p_ind]\n",
      "                del categorylist[p_ind]\n",
      "        dwords=dwords[0:len(dwords)-1]+'];';\n",
      "        fnames = fnames[0:len(fnames)-1]+'];';\n",
      "        links = links[0:len(links)-1]+'];';\n",
      "        values = values[0:len(values)-1]+'];';\n",
      "        fvalues = fvalues[0:len(fvalues)-1]+'];';\n",
      "        topic_num = 0\n",
      "        for wpc,tpc in zip(word_pcs,title_pcs):\n",
      "            outputfile.write( '***** TOPIC %d *****\\n' % topic_num)\n",
      "            for row in wpc:\n",
      "                outputfile.write(row+'\\n')\n",
      "            outputfile.write('-----\\n')\n",
      "            for row in tpc:\n",
      "                outputfile.write(row+'\\n')\n",
      "            outputfile.write('\\n')\n",
      "            topic_num = topic_num + 1\n",
      "        outputfile = open(r'scatterLasso.js','w')\n",
      "        outputfile.write('%s\\n%s\\n%s\\n%s\\n%s\\n'%(dwords, fnames, links, values, fvalues))\n",
      "        print 'Final results:'\n",
      "        print (dwords, fnames, links, values, fvalues)\n",
      "        #print word_pcs\n",
      "        #print title_pcs\n",
      "\n",
      "    def remove_rows(self, inds_to_remove):\n",
      "        self.M = self.M.T\n",
      "        self.remove_cols(inds_to_remove)\n",
      "        self.M = self.M.T\n",
      "    def remove_cols(self, inds_to_remove):\n",
      "        inds_to_remove = sorted(inds_to_remove)\n",
      "        inds_to_remove.reverse()\n",
      "        for ind in inds_to_remove:\n",
      "            self.remove_col(ind)\n",
      "\n",
      "    def remove_col(self, ind):\n",
      "        if ind == 0:\n",
      "            self.M = self.M[:,ind+1:].tocsc()\n",
      "        elif ind == (self.M.shape[1]-1):\n",
      "            self.M = self.M[:,0:ind].tocsc()\n",
      "        else:\n",
      "            self.M = scipy.sparse.hstack([self.M[:,0:ind],self.M[:,ind+1:]]).tocsc()\n",
      "\n",
      "    def ini_mat(self):\n",
      "        p = scipy.sparse.csc_matrix(numpy.ones(shape=(self.M.shape[0],1)))\n",
      "        q = scipy.sparse.csc_matrix(numpy.ones(shape=(self.M.shape[1],1)))\n",
      "        p = p/math.sqrt(p.T.dot(p).data[0])\n",
      "        q = q/math.sqrt(q.T.dot(q).data[0])\n",
      "        return (p,q)\n",
      "\n",
      "Iterator.run('./sparseMatrix.csv', './wordlist.csv', './documentTitle.csv', './hyperlink.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "running\n",
        "handling pc# 0\n",
        "handling pc# 1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "handling pc# 2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Final results:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('var dwords=[[\"level\",\"scheduling\",\"problem\",\"cloud\",\"coordination\"],[\"service\",\"msnp\",\"discovery\",\"scheme\",\"mobile\"],[\"method\",\"om\",\"nystr\",\"matrix\",\"efficient\"]];', 'var fnames=[[\"Hybrid Scheduling/Signal-Level Coordination in the Downlink of Multi-Cloud Radio-Access Networks. (arXiv:1504.01552v1 [cs.IT])\",\"If the Current Clique Algorithms are Optimal, so is Valiant\\'s Parser. (arXiv:1504.01431v1 [cs.CC])\",\"Understanding Soft Errors in Uncore Components. (arXiv:1504.01381v1 [cs.OH])\",\"Grid-free compressive beamforming. (arXiv:1504.01662v1 [cs.IT])\",\"Near-optimal asymmetric binary matrix partitions. (arXiv:1407.8170v2 [cs.GT] UPDATED)\"],[\"Service Discovery and Trust in Mobile Social Network in Proximity. (arXiv:1504.01504v1 [cs.SI])\",\"Exploiting Regional Differences: A Spatially Adaptive Random Access. (arXiv:1403.3891v4 [cs.IT] UPDATED)\",\"Voice based self help System: User Experience Vs Accuracy. (arXiv:1504.01496v1 [cs.CL])\",\"The Performance Analysis of Coded Cache in Wireless Fading Channel. (arXiv:1504.01452v1 [cs.NI])\",\"Energy saving market for mobile operators. (arXiv:1504.01526v1 [cs.NI])\"],[\"Towards More Efficient Nystrom Approximation and CUR Matrix Decomposition. (arXiv:1503.08395v2 [cs.LG] UPDATED)\",\"Efficient SDP Inference for Fully-connected CRFs Based on Low-rank Decomposition. (arXiv:1504.01492v1 [cs.CV])\",\"Simultaneously sparse and low-rank abundance matrix estimation for hyperspectral image unmixing. (arXiv:1504.01515v1 [cs.CV])\",\"Improvement of the image quality of random phase--free holography using an iterative method. (arXiv:1504.01424v1 [physics.optics])\",\"Knowledge driven Offline to Online Script Conversion. (arXiv:1504.01420v1 [cs.CV])\"]];', 'var links=[[\"http://arxiv.org/abs/1504.01552\",\"http://arxiv.org/abs/1504.01431\",\"http://arxiv.org/abs/1504.01381\",\"http://arxiv.org/abs/1504.01662\",\"http://arxiv.org/abs/1407.8170\"],[\"http://arxiv.org/abs/1504.01504\",\"http://arxiv.org/abs/1403.3891\",\"http://arxiv.org/abs/1504.01496\",\"http://arxiv.org/abs/1504.01452\",\"http://arxiv.org/abs/1504.01526\"],[\"http://arxiv.org/abs/1503.08395\",\"http://arxiv.org/abs/1504.01492\",\"http://arxiv.org/abs/1504.01515\",\"http://arxiv.org/abs/1504.01424\",\"http://arxiv.org/abs/1504.01420\"]];', 'var values=[[0.254,0.227,0.206,0.199,0.114],[0.287,0.283,0.198,0.141,0.090],[0.276,0.207,0.207,0.194,0.116]];', 'var fvalues=[[6.727,1.445,1.270,0.826,0.619],[7.788,1.408,0.575,0.422,0.361],[7.895,1.289,0.664,0.553,0.553]];')\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}