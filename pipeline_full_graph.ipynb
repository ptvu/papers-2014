{
 "metadata": {
  "name": "",
  "signature": "sha256:722a4b432824209acad6260d1df40d479f7fdbd3c78ac76f9a5894212995a6b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "import scipy\n",
      "from scipy import *\n",
      "from scipy.sparse import *\n",
      "from scipy.sparse.linalg import *\n",
      "\n",
      "vocab = [line.strip() for line in open('vocab.nips.txt', 'rb')]\n",
      "stop_words = [not str.isalpha(word) for word in vocab]\n",
      "\n",
      "skiprows = 3\n",
      "I = []\n",
      "J = []\n",
      "V = []\n",
      "with open('docword.nips.txt') as csvfile:\n",
      "    line_cnt = 0\n",
      "    for line in csvfile:\n",
      "        line_cnt = line_cnt + 1\n",
      "        if (line_cnt % 10000000 == 0):\n",
      "            print line_cnt\n",
      "        if (line_cnt == 1):\n",
      "            num_rows = int(line.strip())\n",
      "        if (line_cnt == 2):\n",
      "            num_cols = int(line.strip())\n",
      "        if (line_cnt > skiprows):\n",
      "            row = line.strip().split(' ')\n",
      "            r, c, v = [int(e) for e in row]\n",
      "            r = r - 1\n",
      "            c = c - 1\n",
      "            if (not stop_words[c]):\n",
      "                I.append(r)\n",
      "                J.append(c)\n",
      "                V.append(v)\n",
      "\n",
      "A = scipy.sparse.coo_matrix((V, (I, J)), dtype='float')\n",
      "    \n",
      "rank = 11\n",
      "P, D, QT = svds(A, k = rank)\n",
      "save_D = D\n",
      "epsilon = D[0]\n",
      "P = P[:, 1:rank]\n",
      "D = D[1:rank]\n",
      "QT = QT[1:rank, :]\n",
      "\n",
      "S = scipy.sqrt(D)\n",
      "P = P.dot(diag(S))\n",
      "QT = diag(S).dot(QT)\n",
      "print P.shape, QT.shape, epsilon\n",
      "# print scipy.linalg.norm(A - P.dot(QT), ord=2)\n",
      "import pickle\n",
      "pickle.dump( (P, QT, epsilon), open( \"lr.pickle\", \"wb\" ) )\n",
      "# pickle.dump( A, open( \"original.pickle\", \"wb\" ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(1500, 10) (10, 12419) 415.764884226\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Q = P\n",
      "P = QT.T\n",
      "Q.shape, P.shape\n",
      "full_matrix = A.tocsr()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.linalg import *\n",
      "import cvxopt\n",
      "from cvxpy import *\n",
      "    \n",
      "def find_topic_imaging(P, Q, y, lambd = 0.15, epsilon = 10, NUM_WORDS = 10):\n",
      "    lambd = 0.15\n",
      "    epsilon = 10\n",
      "\n",
      "    K = Q.T.dot(Q)\n",
      "\n",
      "    c = inv(sqrtm(K)).dot(Q.T.dot(y))\n",
      "    s = numpy.sqrt(y.T.dot(y) - c.T.dot(c))\n",
      "    R = P.dot(sqrtm(K))\n",
      "\n",
      "    n = P.shape[0]\n",
      "    k = K.shape[0]\n",
      "    w = Variable(n)\n",
      "    objective = Minimize(norm(vstack(c - R.T * w, s), 2) + epsilon * norm(w, 2) + lambd * norm(w, 1))\n",
      "    p = Problem(objective)\n",
      "\n",
      "    import time\n",
      "    start = time.time()\n",
      "    p.solve()\n",
      "    total_time = time.time() - start\n",
      "    weights = sorted(w.value, reverse=True)\n",
      "    order = sorted(range(len(weights)), reverse=True, key=lambda k: w.value[k])\n",
      "\n",
      "    topic_imaging = [(vocab[order[i]], float(w.value[order[i]])) for i in range(NUM_WORDS)]\n",
      "    return {'total_time': total_time,\n",
      "            'topic_imaging' : topic_imaging, \n",
      "            'num_pos_documents' : len(numpy.where(y == 1)[0]), \n",
      "            'num_neg_documents' : len(numpy.where(y == -1)[0])}\n",
      "\n",
      "def make_query(P, Q, query):\n",
      "    feature_query_id = -1\n",
      "    for i, each in enumerate(vocab):\n",
      "        if each == query:\n",
      "            feature_query_id = i\n",
      "    import numpy\n",
      "    label = -numpy.ones([Q.shape[0], 1])\n",
      "    label[ find(full_matrix[:, feature_query_id] != 0)[0] ] = 1\n",
      "    return find_topic_imaging(P, Q, label)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "make_query(P, Q, query = 'optimization')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "{'num_neg_documents': 1132,\n",
        " 'num_pos_documents': 368,\n",
        " 'topic_imaging': [('algorithm', 0.004521082677516274),\n",
        "  ('neural', 2.996882576591139e-10),\n",
        "  ('variables', 1.3103865198490022e-10),\n",
        "  ('recurrent', 1.2513559687316023e-10),\n",
        "  ('weight', 1.0735432086025252e-10),\n",
        "  ('likelihood', 9.307050725089705e-11),\n",
        "  ('distribution', 9.249998574408051e-11),\n",
        "  ('parameter', 8.873127491376296e-11),\n",
        "  ('bound', 8.799658016290256e-11),\n",
        "  ('problem', 7.894390697464088e-11)],\n",
        " 'total_time': 4.083547115325928}"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "full_graph = []\n",
      "for feature_query_id in range(len(vocab)):\n",
      "    query = vocab[feature_query_id]\n",
      "    cur = {}\n",
      "    cur['query'] = query\n",
      "    cur['feature_query_id'] = feature_query_id\n",
      "\n",
      "    if not stop_words[feature_query_id]:\n",
      "        label = -numpy.ones([Q.shape[0], 1])\n",
      "        label[ find(full_matrix[:, feature_query_id] != 0)[0] ] = 1\n",
      "        cur['result'] = find_topic_imaging(P, Q, label)\n",
      "        \n",
      "    # print cur\n",
      "    full_graph.append(cur)\n",
      "    if feature_query_id % 100 == 0:\n",
      "        print cur\n",
      "import pickle\n",
      "pickle.dump( full_graph, open( \"full_graph.pickle\", \"wb\" ) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'query': 'a2i', 'feature_query_id': 0}\n",
        "{'query': 'acid', 'feature_query_id': 100, 'result': {'total_time': 3.545426845550537, 'num_neg_documents': 1480, 'num_pos_documents': 20, 'topic_imaging': [('student', 7.113034580224275e-11), ('recurrent', 5.407115278807605e-11), ('teacher', 5.070116169974326e-11), ('generalization', 4.00489509216718e-11), ('plant', 3.4565299234713814e-11), ('weigend', 3.311245348707522e-11), ('pruning', 3.2909974077146034e-11), ('validation', 3.2552309032261616e-11), ('obd', 3.2238413769267486e-11), ('biases', 2.5447020501375054e-11)]}}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'query': 'admit', 'feature_query_id': 200, 'result': {'total_time': 3.745805025100708, 'num_neg_documents': 1470, 'num_pos_documents': 30, 'topic_imaging': [('generalization', 1.9655464608670343e-10), ('student', 1.8849164932985e-10), ('controller', 1.336416925813851e-10), ('teacher', 1.2786746827771757e-10), ('plant', 1.1744968499620705e-10), ('validation', 1.1402663020852722e-10), ('obd', 9.031291978842302e-11), ('pruning', 8.893249550199201e-11), ('weigend', 8.694861711439564e-11), ('recurrent', 8.110409493317668e-11)]}}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'query': 'alcove', 'feature_query_id': 300, 'result': {'total_time': 3.6354808807373047, 'num_neg_documents': 1495, 'num_pos_documents': 5, 'topic_imaging': [('student', 6.529345235962047e-10), ('generalization', 6.291964669448892e-10), ('teacher', 4.689815747545291e-10), ('recurrent', 3.6088621935397085e-10), ('controller', 3.596042805730149e-10), ('validation', 3.43680005509582e-10), ('plant', 3.328080423884326e-10), ('pruning', 2.7369440643009487e-10), ('weigend', 2.7169669340511835e-10), ('obd', 2.660161512548606e-10)]}}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'query': 'amples', 'feature_query_id': 400, 'result': {'total_time': 3.597700834274292, 'num_neg_documents': 1473, 'num_pos_documents': 27, 'topic_imaging': [('error', 1.6764440201133033e-10), ('training', 1.5713337259851275e-10), ('generalization', 9.91684440790489e-11), ('hidden', 4.3506098896585735e-11), ('validation', 3.747912187526702e-11), ('student', 3.5448987453795434e-11), ('weight', 3.0356444351026295e-11), ('teacher', 2.3504695321903454e-11), ('decay', 1.5966548304153854e-11), ('ensemble', 1.582048055351765e-11)]}}"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "{'query': 'antisymmetric', 'feature_query_id': 500, 'result': {'total_time': 3.2427568435668945, 'num_neg_documents': 1492, 'num_pos_documents': 8, 'topic_imaging': [('student', 1.1140489125170251e-10), ('recurrent', 8.967271674208546e-11), ('teacher', 8.182620828591197e-11), ('controller', 7.659540776522511e-11), ('plant', 6.971072920196572e-11), ('generalization', 5.873432680235437e-11), ('weigend', 5.1187213914316025e-11), ('obd', 5.049939609906197e-11), ('pruning', 4.7554570614951834e-11), ('forward', 4.3038112662196916e-11)]}}"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}